{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertConfig, TrainingArguments, Trainer\n",
    "from CustomBertModel import fixed_predict, DataCollatorForMultiMask\n",
    "from MoELayer import BertWwmMoE\n",
    "from datasets import Dataset\n",
    "from ltp import LTP\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/examples/research_projects/mlm_wwm/run_chinese_ref.py\n",
    "from bert.run_chinese_ref import prepare_ref\n",
    "\n",
    "import random\n",
    "import torch\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ltp = LTP()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Midsummra/CNMBert-MoE\")\n",
    "config = BertConfig.from_pretrained('Midsummra/CNMBert-MoE')\n",
    "model = BertWwmMoE.from_pretrained('Midsummra/CNMBert-MoE', config=config).to('cuda')"
   ],
   "id": "497437f079c218e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 数据预处理\n",
    "\n",
    "text = set()\n",
    "with open('./webtext/train.csv', mode='r', encoding='utf-8') as file:\n",
    "    line = file.readline()\n",
    "    while True:\n",
    "        if not line:\n",
    "            break\n",
    "        text.add(line)\n",
    "        line = file.readline()\n",
    "\n",
    "text = [t.replace('\\n', '') for t in list(text)]\n",
    "random.shuffle(text)\n",
    "\n",
    "train_data = {'text': text[1000:]}\n",
    "eval_data = {'text': text[:1000]}\n"
   ],
   "id": "577b4745dde4c3ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize_func(dataset):\n",
    "    tokens = tokenizer(dataset['text'],\n",
    "                       max_length=64,\n",
    "                       padding='max_length',\n",
    "                       truncation=True,\n",
    "                       return_tensors='pt'\n",
    "                       )\n",
    "    ref = prepare_ref(dataset['text'], ltp, tokenizer)\n",
    "    features = {'input_ids': tokens['input_ids'], 'chinese_ref': ref, 'attention_mask': tokens['attention_mask']}\n",
    "    return features\n",
    "\n",
    "data_collator = DataCollatorForMultiMask(tokenizer,\n",
    "                                             mlm_probability=0.15,\n",
    "                                             mlm=True)\n",
    "\n",
    "train_dataset = train_data.map(tokenize_func, batched=True, remove_columns=[\"text\"])\n",
    "eval_dataset = eval_data.map(tokenize_func, batched=True, remove_columns=[\"text\"])\n"
   ],
   "id": "5d68fd569a379e40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 训练\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = model.to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable layer: {name}\")\n",
    "    param.data = param.data.contiguous()\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./model/checkpoints/',\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=256,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    learning_rate=2e-5,  #学习率建议给1e-5~2e-5\n",
    "    weight_decay=1e-5,\n",
    "    logging_dir='./model/logs/',\n",
    "    logging_steps=100,\n",
    "    logging_first_step=True,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    save_total_limit=4,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=1 / 20\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ],
   "id": "2c9bf9ebb808cf34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer.train()\n",
    "trainer.save_model('./model/cnmbert-ft')\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation cnmbert-ft: {eval_results}\")"
   ],
   "id": "b4004f82f39462d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
