# zh-CN-Multi-Mask-Bert (CNMBertğŸ‹)
~~åƒæŸ æª¬Bert~~
![image](https://github.com/user-attachments/assets/a888fde7-6766-43f1-a753-810399418bda)

---

ä¸€ä¸ªç”¨æ¥ç¿»è¯‘æ‹¼éŸ³ç¼©å†™çš„æ¨¡å‹

æ­¤æ¨¡å‹åŸºäº[Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)è®­ç»ƒè€Œæ¥ï¼Œé€šè¿‡ä¿®æ”¹å…¶é¢„è®­ç»ƒä»»åŠ¡æ¥ä½¿å…¶é€‚é…æ‹¼éŸ³ç¼©å†™ç¿»è¯‘ä»»åŠ¡ï¼Œç›¸è¾ƒäºå¾®è°ƒè¿‡çš„GPTæ¨¡å‹ä»¥åŠGPT-4oè¾¾åˆ°äº†sota

---

## ä»€ä¹ˆæ˜¯æ‹¼éŸ³ç¼©å†™

å½¢å¦‚:

> "bhys" -> "ä¸å¥½æ„æ€"
>
> "kb" -> "çœ‹ç—…"

è¿™æ ·çš„ï¼Œä½¿ç”¨æ‹¼éŸ³é¦–å­—æ¯æ¥ä»£æ›¿æ±‰å­—çš„ç¼©å†™ï¼Œæˆ‘ä»¬å§‘ä¸”ç§°ä¹‹ä¸ºæ‹¼éŸ³ç¼©å†™ã€‚

å¦‚æœå¯¹æ‹¼éŸ³ç¼©å†™æ„Ÿå…´è¶£å¯ä»¥çœ‹çœ‹è¿™ä¸ªâ†“

[å¤§å®¶ä¸ºä»€ä¹ˆä¼šè®¨åŒç¼©å†™ï¼Ÿ - è¿œæ–¹é’æœ¨çš„å›ç­” - çŸ¥ä¹](https://www.zhihu.com/question/269016377/answer/2654824753)

### CNMBert

| Model           | æ¨¡å‹æƒé‡                                                    | Memory Usage (FP16) | Model Size | QPS   | MRR   | Acc   |
| --------------- | ----------------------------------------------------------- | ------------------- | ---------- | ----- | ----- | ----- |
| CNMBert-Default* | [Huggingface](https://huggingface.co/Midsummra/CNMBert)     | 0.4GB               | 131M       | 12.56 | 59.70 | 49.74 |
| CNMBert-MoE     | [Huggingface](https://huggingface.co/Midsummra/CNMBert-MoE) | 0.8GB               | 329M       | 3.20  | 61.53 | 51.86 |

* æ‰€æœ‰æ¨¡å‹å‡åœ¨ç›¸åŒçš„200ä¸‡æ¡wikiä»¥åŠçŸ¥ä¹è¯­æ–™ä¸‹è®­ç»ƒ
* QPS ä¸º queries per second 
* MRR ä¸ºå¹³å‡å€’æ•°æ’å(mean reciprocal rank)
* Acc ä¸ºå‡†ç¡®ç‡(accuracy)
* CNMBert-Default å­˜åœ¨[é‡åŒ–ç‰ˆæœ¬](https://huggingface.co/mradermacher/CNMBert-GGUF)

æ¨¡å‹æ¶æ„&æ€§èƒ½å¯¹æ¯”:
![overall (1)](https://github.com/user-attachments/assets/cf9575c4-c37d-484b-8a3b-f8f536ca78c9)
![output](https://github.com/user-attachments/assets/3de2b56d-f8cb-40f1-8ffa-68968bbd2ed5)


### Usage

```python
from transformers import AutoTokenizer, BertConfig

from CustomBertModel import predict
from MoELayer import BertWwmMoE
```

åŠ è½½æ¨¡å‹

```python
# use CNMBert with MoE
# To use CNMBert without MoE, replace all "Midsummra/CNMBert-MoE" with "Midsummra/CNMBert" and use BertForMaskedLM instead of using BertWwmMoE
tokenizer = AutoTokenizer.from_pretrained("Midsummra/CNMBert-MoE")
config = BertConfig.from_pretrained('Midsummra/CNMBert-MoE')
model = BertWwmMoE.from_pretrained('Midsummra/CNMBert-MoE', config=config).to('cuda')

# model = BertForMaskedLM.from_pretrained('Midsummra/CNMBert').to('cuda')
```

é¢„æµ‹è¯è¯­

```python
print(predict("æˆ‘æœ‰ä¸¤åƒkq", "kq", model, tokenizer)[:5])
print(predict("å¿«å»ç»™é­”ç†æ²™çœ‹bå§", "b", model, tokenizer[:5]))
```

> ['å—é’±', 1.2056937473156175], ['å—å‰', 0.05837443749364857], ['å¼€åƒ', 0.0483869208528063], ['å¯åƒ', 0.03996622172280445], ['å£æ°”', 0.037183335575008414]

> ['ç—…', 1.6893256306648254], ['å§', 0.1642467901110649], ['å‘—', 0.026976384222507477], ['åŒ…', 0.021441461518406868], ['æŠ¥', 0.01396679226309061]

---

```python
# é»˜è®¤çš„predictå‡½æ•°ä½¿ç”¨æŸæœç´¢
def predict(sentence: str, 
            predict_word: str,
            model,
            tokenizer,
            top_k=8,
            beam_size=16, # æŸå®½
            threshold=0.005, # é˜ˆå€¼
            fast_mode=True, # æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæ¨¡å¼
            strict_mode=True): # æ˜¯å¦å¯¹è¾“å‡ºç»“æœè¿›è¡Œæ£€æŸ¥
            
# ä½¿ç”¨å›æº¯çš„æ— å‰ªææš´åŠ›æœç´¢
def backtrack_predict(sentence: str,
            predict_word: str,
            model,
            tokenizer,
            top_k=10,
            fast_mode=True,
            strict_mode=True):
```

> ç”±äºBERTçš„è‡ªç¼–ç ç‰¹æ€§ï¼Œå¯¼è‡´å…¶åœ¨é¢„æµ‹MASKæ—¶ï¼Œé¡ºåºä¸åŒä¼šå¯¼è‡´é¢„æµ‹ç»“æœä¸åŒï¼Œå¦‚æœå¯ç”¨`fast_mode`ï¼Œåˆ™ä¼šæ­£å‘å’Œåå‘åˆ†åˆ«å¯¹è¾“å…¥è¿›è¡Œé¢„æµ‹ï¼Œå¯ä»¥æå‡ä¸€ç‚¹å‡†ç¡®ç‡(2%å·¦å³)ï¼Œä½†æ˜¯ä¼šå¸¦æ¥æ›´å¤§çš„æ€§èƒ½å¼€é”€ã€‚

> `strict_mode`ä¼šå¯¹è¾“å…¥è¿›è¡Œæ£€æŸ¥ï¼Œä»¥åˆ¤æ–­å…¶æ˜¯å¦ä¸ºä¸€ä¸ªçœŸå®å­˜åœ¨çš„æ±‰è¯­è¯æ±‡ã€‚

### å¦‚ä½•å¾®è°ƒæ¨¡å‹

è¯·å‚è€ƒ[TrainExample.ipynb](https://github.com/IgarashiAkatuki/CNMBert/blob/main/TrainExample.ipynb),åœ¨æ•°æ®é›†çš„æ ¼å¼ä¸Šï¼Œåªè¦ä¿è¯csvçš„ç¬¬ä¸€åˆ—ä¸ºè¦è®­ç»ƒçš„è¯­æ–™å³å¯ã€‚

### Q&A

Q: æ„Ÿè§‰è¿™ä¸ªä¸œè¥¿å‡†ç¡®åº¦æœ‰ç‚¹ä½å•Š

A: å¯ä»¥å°è¯•è®¾ç½®`fast_mode`å’Œ`strict_mode`ä¸º`False`ã€‚ æ¨¡å‹æ˜¯åœ¨å¾ˆå°çš„æ•°æ®é›†(200w)ä¸Šè¿›è¡Œçš„é¢„è®­ç»ƒï¼Œæ‰€ä»¥æ³›åŒ–èƒ½åŠ›ä¸è¶³å¾ˆæ­£å¸¸ï¼Œï¼Œï¼Œå¯ä»¥åœ¨æ›´å¤§æ•°æ®é›†æˆ–è€…æ›´åŠ ç»†åˆ†çš„é¢†åŸŸè¿›è¡Œå¾®è°ƒï¼Œå…·ä½“å¾®è°ƒæ–¹å¼å’Œ[Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)å·®åˆ«ä¸å¤§ï¼Œåªéœ€è¦å°†`DataCollactor`æ›¿æ¢ä¸º`CustomBertModel.py`ä¸­çš„`DataCollatorForMultiMask`ã€‚

### å¼•ç”¨
å¦‚æœæ‚¨å¯¹CNMBertçš„å…·ä½“å®ç°æ„Ÿå…´è¶£çš„è¯ï¼Œå¯ä»¥å‚è€ƒ
```
@misc{feng2024cnmbertmodelhanyupinyin,
      title={CNMBert: A Model For Hanyu Pinyin Abbreviation to Character Conversion Task}, 
      author={Zishuo Feng and Feng Cao},
      year={2024},
      eprint={2411.11770},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.11770}, 
}
```

